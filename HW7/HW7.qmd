---
title: "Homework 7"
format: html
editor: visual
---

```{r include = FALSE}
library(tidyverse)
library(magrittr)
library(ISLR2)
library(ggplot2)
library(GLMsData)
library(car)
library(GGally)
library(faraway)
library(pander)
library(readr)
library(dplyr)
library(readxl)
library(corrplot)
library(interactions)
library(leaps)
library(glmnet)
library(tree)
library(randomForest) 
library(gbm) 
library(BART) 
```

## Homework 7

### Question 8

#### Part A)

```{r}
Carseats
summary(Carseats)
dim(Carseats)
```

```{r}
set.seed(4)

train = sample(1:nrow(Carseats), nrow(Carseats) / 2) #200 row for training, 200 for testing

Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
```

#### Part B)

```{r}
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
```

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

ShelveLoc is our first leaf of our tree, Then our tree is branched according to the price and age. Then next leaves created our decision tree. ShelveLoc, Price, and age are our most important predictors.

```{r}
pred.carseats = predict(tree.carseats, Carseats.test)
mse <- mean((Carseats.test$Sales - pred.carseats)^2)
mse
```

MSE is 6.33

#### Part C)

```{r}
set.seed(4)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size,cv.carseats$dev,type="b")

min_error <- min(cv.carseats$dev)
best_size <- cv.carseats$size[which.min(cv.carseats$dev)]
abline(v = best_size, col = "red", lty = 2, lwd = 2) 
points(best_size, min_error, col = "red", pch = 19)
```

Best size seems to 11 nodes.

```{r}
prune.carseats = prune.tree(tree.carseats, best = 11)
par(mfrow = c(1, 1))
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

```{r}
pred.prune = predict(prune.carseats, Carseats.test)
mse_prune <- mean((Carseats.test$Sales - pred.prune)^2)
mse_prune
```

When we prune the tree, MSE increases to 6.4

#### Part E)

I chose m as the 1/3 of \# of predictors. i.e. 10/3

```{r}
set.seed(4)
rf.carseats = randomForest(Sales ~ ., data = Carseats.train, mtry = 3, importance = TRUE)
rf.pred = predict(rf.carseats, Carseats.test)
mse_rf <- mean((Carseats.test$Sales - rf.pred)^2)
mse_rf
```

When we make random forest, MSE drops to 3.61

```{r}
importance(rf.carseats)
```

We see that ShelveLoc and Price are our most important predictors.

```{r}
rf.carseats1 = randomForest(Sales ~ ., data = Carseats.train, mtry = 1, importance = TRUE)
rf.pred1 = predict(rf.carseats1, Carseats.test)
mean((Carseats.test$Sales - rf.pred1)^2)
```

When we decrease m, MSE increases

```{r}
rf.carseats2 = randomForest(Sales ~ ., data = Carseats.train, mtry = 7, importance = TRUE)
rf.pred2 = predict(rf.carseats2, Carseats.test)
mean((Carseats.test$Sales - rf.pred2)^2)
```

When we decrease m, MSE decreases too. However, large values of m causes there will not be as much randomness introduced into the model.

### Car Dealer Question

```{r include = FALSE}
d <- read_csv("~/Desktop/used-cars.csv", show_col_types = FALSE) %>% 
  select(Price, Age_08_04, KM, Fuel_Type, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight) %>% 
  rename(Age = Age_08_04, 
         Fuel = Fuel_Type,
         Metallic = Met_Color,
         QuartTax = Quarterly_Tax) %>% 
  mutate(Fuel = factor(Fuel),
         Metallic = factor(Metallic, levels = c(0,1), labels = c("No", "Yes")),
         Automatic = factor(Automatic, levels = c(0,1), labels = c("No", "Yes")),
         across(c(Doors, CC, HP, QuartTax), factor)
         )
d_noNa<-na.omit(d)
```

```{r}
d_noNa
summary(d_noNa)
dim(d_noNa)
```

#### Part 1)

```{r}
set.seed(2)

train_car <- sample(1:nrow(d_noNa), nrow(d_noNa) / 2) #718 row for training, 718 for testing

car.train <- d_noNa[train_car, ]
car.test <- d_noNa[-train_car, ]
```

```{r}
tree.car <- tree(Price ~ ., data = car.train)
summary(tree.car)
```

```{r}
plot(tree.car)
text(tree.car, pretty = 0)
```

I chose m as the 1/3 of \# of predictors. i.e. 10/3

```{r}
set.seed(2)
rf.car <- randomForest(Price ~ ., data = car.train, mtry = 3, importance = TRUE)
summary(rf.car)
```

#### Part 2)

```{r}
rf.predCar <- predict(rf.car, car.test)
mse_rf_car <- mean((car.test$Price - rf.predCar)^2)
mse_rf_car
```

```{r}
tree.predCar <- predict(tree.car, car.test)
mse_tree_car <- mean((car.test$Price - tree.predCar)^2)
mse_tree_car
```

To calculate confidence intervals for Mean Squared Error (MSE) in decision tree and random forest models, employ bootstrapping or cross-validation techniques to estimate the MSE distribution and derive intervals.

#### Part 3)

NO PART 3

# IN GENERAL, THE WAY OF ASKING THE SECOND QUESTION IS NOT VERY CLEAR. MOST OF MY FRIENDS DID NOT UNDERSTAND WHAT IS EXPECTED FROM US EVEN THOUGH WE SPENT A LOTS OF TIME TO COMPREHEND. IT COULD BE WRITTEN IN A MORE PRECISE WAY. 
